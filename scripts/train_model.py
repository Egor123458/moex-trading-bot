#!/usr/bin/env python3
"""–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è ML-–º–æ–¥–µ–ª–∏"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data_collection.database import DatabaseManager
from src.ml_models.features.feature_engineering import FeatureEngineer
from src.ml_models.models.xgboost_model import XGBoostClassifier
from config.settings import settings
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def train_optimized_model():
    """–û–±—É—á–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    
    print("="*60)
    print("–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï")
    print("="*60)
    
    tickers = ['SBER', 'GAZP', 'LKOH', 'GMKN']
    timeframe = '1h'
    
    horizons = [5, 10, 15]
    threshold = 0.015
    
    print(f"\n–¢–∏–∫–µ—Ä—ã: {tickers}")
    print(f"–¢–∞–π–º—Ñ—Ä–µ–π–º: {timeframe}")
    print(f"–ì–æ—Ä–∏–∑–æ–Ω—Ç—ã: {horizons}")
    print(f"–ü–æ—Ä–æ–≥ —Ä–æ—Å—Ç–∞: {threshold*100:.1f}%")
    
    print("\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î...")
    db = DatabaseManager(settings.db.DATABASE_URL)
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–∫–µ—Ä—É
    ticker_data = {}
    
    feature_eng = FeatureEngineer()
    
    for ticker in tickers:
        print(f"\n  –û–±—Ä–∞–±–æ—Ç–∫–∞ {ticker}...")
        data = db.load_candles(ticker, timeframe, start_date, end_date)
        
        if data.empty:
            print(f"    ‚úó –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}")
            continue
        
        print(f"    –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å–≤–µ—á–µ–π")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features = feature_eng.create_features(data)
        
        if len(features) > 0:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫ —Ç–∏–∫–µ—Ä–∞
            for t in tickers:
                features[f'is_{t}'] = int(ticker == t)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            ticker_data[ticker] = {
                'features': features,
                'data': data
            }
            
            print(f"    ‚úì –°–æ–∑–¥–∞–Ω–æ {len(features.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, {len(features)} —Å—Ç—Ä–æ–∫")
    
    best_model = None
    best_auc = 0
    best_horizon = 0
    
    # –û–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
    for horizon in horizons:
        print("\n" + "="*60)
        print(f"–ì–û–†–ò–ó–û–ù–¢: {horizon} –ø–µ—Ä–∏–æ–¥–æ–≤ (~{horizon} —á–∞—Å–æ–≤)")
        print(f"–¶–µ–ª—å: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ä–æ—Å—Ç >{threshold*100:.1f}%")
        print("="*60)
        
        all_X = []
        all_y = []
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ —Å–æ–∑–¥–∞–µ–º labels —Å —É—á–µ—Ç–æ–º –µ–≥–æ –∏–Ω–¥–µ–∫—Å–æ–≤
        for ticker, data_dict in ticker_data.items():
            features = data_dict['features']
            raw_data = data_dict['data']
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º time –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –¥–ª—è labels
            if 'time' in raw_data.columns:
                raw_data = raw_data.set_index('time')
            
            # –°–æ–∑–¥–∞–µ–º labels –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            future_returns = raw_data['close'].shift(-horizon) / raw_data['close'] - 1
            labels = (future_returns > threshold).astype(int)
            
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º features (–∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –æ—Ç NaN)
            common_idx = features.index.intersection(labels.index)
            
            X_ticker = features.loc[common_idx]
            y_ticker = labels.loc[common_idx]
            
            # –£–¥–∞–ª—è–µ–º NaN –∏–∑ labels (–∏–∑-–∑–∞ shift)
            valid_mask = ~y_ticker.isna()
            X_ticker = X_ticker[valid_mask]
            y_ticker = y_ticker[valid_mask]
            
            all_X.append(X_ticker)
            all_y.append(y_ticker)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        X = pd.concat(all_X, axis=0)
        y = pd.concat(all_y, axis=0)
        
        print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} —Å—ç–º–ø–ª–æ–≤")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        print(f"  –°–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç (>{threshold*100:.1f}%): {y.sum()} ({y.sum()/len(y)*100:.1f}%)")
        print(f"  –û—Å—Ç–∞–ª—å–Ω–æ–µ: {len(y)-y.sum()} ({(len(y)-y.sum())/len(y)*100:.1f}%)")
        
        if y.sum() < len(y) * 0.2:
            print(f"\n‚ö†Ô∏è –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤! –ò—Å–ø–æ–ª—å–∑—É–µ–º scale_pos_weight")
            scale_pos_weight = (len(y) - y.sum()) / y.sum()
        else:
            scale_pos_weight = 1.0
        
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'max_depth': 4,
            'learning_rate': 0.03,
            'n_estimators': 500,
            'subsample': 0.7,
            'colsample_bytree': 0.7,
            'colsample_bylevel': 0.7,
            'min_child_weight': 5,
            'gamma': 0.2,
            'reg_alpha': 0.2,
            'reg_lambda': 2.0,
            'scale_pos_weight': scale_pos_weight,
            'random_state': 42
        }
        
        model = XGBoostClassifier(params=params)
        
        print("\n3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        print("-"*60)
        model.train(X, y, test_size=0.2)
        
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import roc_auc_score, precision_recall_curve
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, y_pred_proba)
        
        precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
        best_threshold_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_threshold_idx] if best_threshold_idx < len(thresholds) else 0.5
        
        print(f"\nüìä ROC-AUC: {auc:.4f}")
        print(f"üìä –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_threshold:.3f}")
        
        if auc > best_auc:
            best_auc = auc
            best_model = model
            best_horizon = horizon
            print(f"\n‚úì –≠—Ç–æ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å!")
        
        model_path = f'data/models/MULTI_xgboost_{timeframe}_h{horizon}_t{int(threshold*1000)}.pkl'
        Path('data/models').mkdir(parents=True, exist_ok=True)
        model.save(model_path)
        print(f"\n‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        import json
        threshold_info = {
            'optimal_threshold': float(best_threshold),
            'roc_auc': float(auc),
            'horizon': horizon,
            'growth_threshold': threshold
        }
        
        threshold_path = f'data/models/MULTI_xgboost_{timeframe}_h{horizon}_t{int(threshold*1000)}_info.json'
        with open(threshold_path, 'w') as f:
            json.dump(threshold_info, f, indent=2)
    
    print("\n" + "="*60)
    print("–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("="*60)
    print(f"–õ—É—á—à–∏–π –≥–æ—Ä–∏–∑–æ–Ω—Ç: {best_horizon} –ø–µ—Ä–∏–æ–¥–æ–≤")
    print(f"–õ—É—á—à–∏–π ROC-AUC: {best_auc:.4f}")
    
    if best_auc > 0.55:
        print(f"\n‚úì‚úì‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç –û–¢–õ–ò–ß–ù–´–ô! ROC-AUC > 0.55")
    elif best_auc > 0.52:
        print(f"\n‚úì‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç –•–û–†–û–®–ò–ô. ROC-AUC –º–µ–∂–¥—É 0.52-0.55")
    elif best_auc > 0.50:
        print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç –ü–†–ò–ï–ú–õ–ï–ú–´–ô. ROC-AUC –º–µ–∂–¥—É 0.50-0.52")
    else:
        print(f"\n‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –°–õ–ê–ë–´–ô. ROC-AUC < 0.50")
    
    best_model_path = f'data/models/MULTI_xgboost_{timeframe}_BEST.pkl'
    best_model.save(best_model_path)
    print(f"\n‚úì –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
    
    print("\n" + "="*60)
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*60)

if __name__ == "__main__":
    train_optimized_model()
